{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers by Hand\n",
    "\n",
    "Transformers are behind many of the most exciting recent developments in machine learning. \n",
    "However they are difficult to understand and most attempt to do so attempted to dissect \n",
    "trained models. \n",
    "The goal here is the opposite: we will put the weights into the models by hand, so that \n",
    "we know precisely what they do.\n",
    "\n",
    "The family of models we have chosen here is very similar to GTP-2, but of course much smaller.\n",
    "They will be used to complete text, and generate patterns that are more and more complex as we go.\n",
    "\n",
    "The machine we are going to tweak is the following, \n",
    "where every orange bit is a parameter that can be changed.\n",
    "\n",
    "![transformer](images/transformer.jpg)\n",
    "\n",
    "Note: I'll make it cleaner/clearer and probably computer drawn later.\n",
    "\n",
    "\n",
    "## Metric\n",
    "\n",
    "There are 10 exercises, each corresponding to a function that must implement the transformer.\n",
    "\n",
    "The score for each exercise is derived from:\n",
    "- $P_i$, the number of tests that it solves correctly. There are 100 tests per exercise.\n",
    "- $\\hat P_i$, how much the model is better than random. Precisely, \n",
    "    if there are $|\\Sigma|$ letters in the vocabulary, then $\\hat P_i = \\max(0, \\frac{P_i - 100/|\\Sigma|}{1 - 1/|\\Sigma|})$.\n",
    "- $S_i$ is the number of parameters of the transformer\n",
    "\n",
    "The total score is then\n",
    "$$\n",
    "    \\mathbb S = \\sum_{i=1}^{10}  \\hat P_i + \\frac{100\\,000 * i^2}{S_i} \\cdot (P_i > 95)\n",
    "$$ \n",
    "\n",
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "00-LastChar\n",
       "Complete the text by repeating the last character.\n",
       "\n",
       "Alphabet: 0: ''  1: a  2: b  3: c\n",
       "Input length: 3\n",
       "Examples:\n",
       "    b → b\n",
       "   bb → b\n",
       "    b → b\n",
       "  ccb → b\n",
       "    a → a"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import Tensor\n",
    "from exos import *\n",
    "\n",
    "exo0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Those probably could be explained one at a time, just before the exercise they are needed for.\n",
    "\n",
    "\n",
    "## Parameters\n",
    "\n",
    "All of those parameters can be chosen the way you want, except $\\Sigma$ and $T$, which is given by the exercise.\n",
    "\n",
    "Hyperparameters:\n",
    "- $\\Sigma$, the vocabulary given by the exercise. $|\\Sigma|$ is the size of the vocabulary.\n",
    "- $t$, the number of tokens per prompt, given by the exercise.\n",
    "- $d$: The depth, or number of layers of the transformer\n",
    "- $h$: The number of heads in each layer\n",
    "- $o$: The dimension of output of each head. The size of the embeding and the total dimension of the output of each layer is then $e = h * o$\n",
    "\n",
    "Parameters:\n",
    "- $E \\in \\mathbb R^{|\\Sigma| \\times e}$: The embedding matrix\n",
    "- $U \\in \\mathbb R^{e \\times |\\Sigma|}$: The unembedding matrix\n",
    "- $P \\in \\mathbb R^{t \\times e}$: The positional encoding matrix\n",
    "- For each layer $l$:\n",
    "    + For each attention head $i$:\n",
    "        - $Q^{li} \\in \\mathbb R^{e \\times o}$: The query matrix\n",
    "        - $K^{li} \\in \\mathbb R^{e \\times o}$: The key matrix\n",
    "        - $V^{li} \\in \\mathbb R^{e \\times o}$: The value matrix\n",
    "    + $W^l \\in \\mathbb R^{e \\times e}$: The weight matrix that combines the outputs of each head of the layer\n",
    "    + $FF^l$: a feedforward neural network of input and output of size $e$. This is \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00-LastChar\n",
      "Complete the text by repeating the last character.\n",
      "\n",
      "Alphabet: 0: ''  1: a  2: b  3: c\n",
      "Input length: 3\n",
      "Examples:\n",
      "   cc → c\n",
      "  abb → b\n",
      "   cc → c\n",
      "   aa → a\n",
      "    a → a\n",
      "\n",
      "embedding = Tensor([\n",
      "    [0.0, 0.0, 0.0, 0.0],\n",
      "    [0.0, 0.0, 0.0, 0.0],\n",
      "    [0.0, 0.0, 0.0, 0.0],\n",
      "    [0.0, 0.0, 0.0, 0.0]])\n",
      "unembedding = Tensor([\n",
      "    [0.0, 0.0, 0.0, 0.0],\n",
      "    [0.0, 0.0, 0.0, 0.0],\n",
      "    [0.0, 0.0, 0.0, 0.0],\n",
      "    [0.0, 0.0, 0.0, 0.0]])\n",
      "pos_encoder = Tensor([\n",
      "    [0.0, 0.0, 0.0, 0.0],\n",
      "    [0.0, 0.0, 0.0, 0.0],\n",
      "    [0.0, 0.0, 0.0, 0.0]])\n",
      "\n",
      "layers = []\n"
     ]
    }
   ],
   "source": [
    "print(EXOS[0])\n",
    "\n",
    "print(EXOS[0].print_template(0, 1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  a → a \t'': 0.17  a: 0.48  b: 0.17  c: 0.17\n",
      "  c → c \t'': 0.17  a: 0.17  b: 0.17  c: 0.48\n",
      "bab → b \t'': 0.17  a: 0.17  b: 0.48  c: 0.17\n",
      " bc → c \t'': 0.17  a: 0.17  b: 0.17  c: 0.48\n",
      " aa → a \t'': 0.17  a: 0.48  b: 0.17  c: 0.17\n",
      "cac → c \t'': 0.17  a: 0.17  b: 0.17  c: 0.48\n",
      " ab → b \t'': 0.17  a: 0.17  b: 0.48  c: 0.17\n",
      " ca → a \t'': 0.17  a: 0.48  b: 0.17  c: 0.17\n",
      " cb → b \t'': 0.17  a: 0.17  b: 0.48  c: 0.17\n",
      "  b → b \t'': 0.17  a: 0.17  b: 0.48  c: 0.17\n",
      "Loss: 1.17  Accuracy: 100 / 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 1.169805645942688)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tokens = exo0.vocab_size  # 4\n",
    "\n",
    "embedding = Tensor([\n",
    "    [1.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 1.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 1.0]])\n",
    "unembedding = Tensor([\n",
    "    [1.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 1.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 1.0]])\n",
    "pos_encoder = Tensor([\n",
    "    [0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0]])\n",
    "\n",
    "layers = []\n",
    "\n",
    "EXOS[0].test_model(0, 1, 4, embedding, unembedding, pos_encoder, layers, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (100x2 and 4x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/diego/ai/hand-transformer/hackaton.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/diego/ai/hand-transformer/hackaton.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m pos_encoder \u001b[39m=\u001b[39m Tensor([\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diego/ai/hand-transformer/hackaton.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     [\u001b[39m0.0\u001b[39m, \u001b[39m0.0\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diego/ai/hand-transformer/hackaton.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     [\u001b[39m0.0\u001b[39m, \u001b[39m0.0\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diego/ai/hand-transformer/hackaton.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     [\u001b[39m0.0\u001b[39m, \u001b[39m0.0\u001b[39m]])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/diego/ai/hand-transformer/hackaton.ipynb#X14sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m layers \u001b[39m=\u001b[39m []\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/diego/ai/hand-transformer/hackaton.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m EXOS[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mtest_model(\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m4\u001b[39;49m, embedding, unembedding, pos_encoder, layers, \u001b[39m100\u001b[39;49m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/ml-for-good-VsJxISB_-py3.8/lib/python3.8/site-packages/typeguard/__init__.py:1033\u001b[0m, in \u001b[0;36mtypechecked.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1031\u001b[0m memo \u001b[39m=\u001b[39m _CallMemo(python_func, _localns, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m   1032\u001b[0m check_argument_types(memo)\n\u001b[0;32m-> 1033\u001b[0m retval \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1034\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1035\u001b[0m     check_return_type(retval, memo)\n",
      "File \u001b[0;32m~/ai/hand-transformer/hand.py:225\u001b[0m, in \u001b[0;36mExercise.test_model\u001b[0;34m(self, depth, heads, inner_size, embedding, unembedding, position_encoder, layers, nb_tests)\u001b[0m\n\u001b[1;32m    222\u001b[0m xs_enc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mencode(\u001b[39mlist\u001b[39m(xs))\n\u001b[1;32m    223\u001b[0m ys_enc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mencode(\u001b[39mlist\u001b[39m(ys))[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 225\u001b[0m pred_enc \u001b[39m=\u001b[39m model(xs_enc)\n\u001b[1;32m    226\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mcross_entropy(pred_enc, ys_enc)\u001b[39m.\u001b[39mitem()\n\u001b[1;32m    227\u001b[0m correct \u001b[39m=\u001b[39m (pred_enc\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m ys_enc)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/ml-for-good-VsJxISB_-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ai/hand-transformer/hand.py:145\u001b[0m, in \u001b[0;36mAttentionOnlyTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    142\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks(with_pos)\n\u001b[1;32m    143\u001b[0m out \u001b[39m=\u001b[39m x[:,\n\u001b[1;32m    144\u001b[0m         \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)  \u001b[39m# only the last token is used for prediction\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m unembeded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munembedding(out)\n\u001b[1;32m    146\u001b[0m probas \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msoftmax(unembeded, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    147\u001b[0m \u001b[39mreturn\u001b[39;00m probas\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/ml-for-good-VsJxISB_-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/ml-for-good-VsJxISB_-py3.8/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (100x2 and 4x2)"
     ]
    }
   ],
   "source": [
    "embedding = Tensor([\n",
    "    [0.0, 0.0],\n",
    "    [0.0, 1.0],\n",
    "    [1.0, 1.0],\n",
    "    [1.0, 0.0]])\n",
    "unembedding = Tensor([\n",
    "    [0.0, 0.0, 1.0, 1.0],\n",
    "    [0.0, 1.0, 1.0, 0.0]])\n",
    "pos_encoder = Tensor([\n",
    "    [0.0, 0.0],\n",
    "    [0.0, 0.0],\n",
    "    [0.0, 0.0]])\n",
    "\n",
    "layers = []\n",
    "EXOS[0].test_model(0, 1, 4, embedding, unembedding, pos_encoder, layers, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00-LastChar \t Complete the text by repeating the last character.\n",
      "01-CycleTwo \t Complete the text by repeating the second-last character.\n",
      "02-FirstChar \t Complete the text by repeating the first character.\n",
      "    Note: the first character is not always at the same position,\n",
      "    since inputs have variable length.\n",
      "03-Reverse \t Complete the text by reversing the input after the bar \"|\".\n",
      "04-Difference \t Complete by 0 if the two digits are different and by 1 if they are the same.\n",
      "05-AllTheSame \t Complete by 1 if all the digits are the same and by 0 otherwise.\n",
      "06-KinderAdder \t Complete by the sum of the two digits.\n",
      "    Note: no input will use digits 3 and 4.\n",
      "07-LengthParity \t Complete by 0 if the input length is even and by the empty token otherwise.\n",
      "08-Min \t Complete by the minimum of the four digits.\n",
      "09-ARecall \t Complete with the token following the last A.\n"
     ]
    }
   ],
   "source": [
    "for exo in EXOS:\n",
    "    print(exo.name, \"\\t\", exo.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random tips:\n",
    "- to please the softmax, you can use large weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ml-for-good-VsJxISB_-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16 (default, Dec 18 2022, 19:37:05) \n[GCC 12.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9109232a12c23aa2d2a2b0d0f0f5ecf51bb6c1649b2c631b11ed83b4e2c67960"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
